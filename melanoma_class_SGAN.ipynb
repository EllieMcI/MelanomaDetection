{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the Semi-Supervised GAN for Melanoma Image Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import PIL\n",
    "import torchvision\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "from os import listdir\n",
    "from os.path import join\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "from PIL import Image\n",
    "\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set path to data folders\n",
    "path = os.getcwd()\n",
    "parent = os.path.abspath(os.path.join(path, os.pardir))\n",
    "\n",
    "transform = transforms.ToTensor()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up an Image Pipeline\n",
    "\n",
    "Preparing a data pipeline that handles the pre-processing and augmentation of the labele dataset. Dataset classes are custom classes inheriting from the abstract Pytorch class torch.utils.data.Dataset, overiding the `__len__` and `__getitem__` methods in order to be able toget the size of the dataset via `len(dataset)` and access each datasample as `dataset[index]`. If no transform specified, datasets returning images in the form of objects of type `PIL.PngImagePlugin.PngImageFile`. Turn the images into Pytorch tensors via the Dataset class transform argument."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unlabeled Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_image(filename):\n",
    "    return any(filename.endswith(extension) for extension in [\".jpg\", \".jpeg\", \".png\"])\n",
    "\n",
    "class UnlabeledDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dir_path, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dir_path (string): Directory containing the images.\n",
    "            transform (optional): Transform to be applied on an image.\n",
    "        \"\"\"\n",
    "\n",
    "        self.transform = transform\n",
    "        \n",
    "        # The list of all the image file names (but not the images themselves!) will be read\n",
    "        # when the Dataset object is initialized\n",
    "        self.image_filenames = [join(dir_path, f) for f in listdir(dir_path) if is_image(f)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        img_name = self.image_filenames[idx]\n",
    "        \n",
    "        # Here is where the image actually gets read:\n",
    "        image = Image.open(img_name)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in the unlabeled set:  7018\n"
     ]
    }
   ],
   "source": [
    "unlabeled_data_path = os.path.join(path, parent, 'Data/unlabeled/')\n",
    "unlabeled_set = UnlabeledDataset(dir_path= unlabeled_data_path, transform=transform)\n",
    "\n",
    "print('Number of images in the unlabeled set: ', len(unlabeled_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Labeled Data Set\n",
    "\n",
    "Benign images are named *\\_0.jpg*, whereas images of melanoma have filenames ending with *\\_1.jpg*. The __getitem__ method returns the image with a label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabeledDataset(Dataset):\n",
    "\n",
    "    def __init__(self, dir_path, transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dir_path (string): Directory containing the images.\n",
    "            transform (optional): Optional transform to be applied\n",
    "                on an image.\n",
    "                \n",
    "            If not transform PIL.PngImagePlugin.PngImageFile    \n",
    "        \"\"\"\n",
    "\n",
    "        self.transform = transform\n",
    "        \n",
    "        # The list of all the image file names (but not the images themselves!) will be read\n",
    "        # when the Dataset object is initialized\n",
    "        self.image_filenames = [join(dir_path, f) for f in listdir(dir_path) if is_image(f)]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        img_name = self.image_filenames[idx]\n",
    "        \n",
    "        # Here is where the image actually gets read:\n",
    "        image = Image.open(img_name)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        label = int(img_name[-5])\n",
    "\n",
    "        return image, label\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of images in the labeled set:  200\n"
     ]
    }
   ],
   "source": [
    "labeled_data_path = os.path.join(path, parent, 'Data/labeled/')\n",
    "labeled_set = LabeledDataset(dir_path= labeled_data_path, transform=transform)\n",
    "\n",
    "print('Number of images in the labeled set: ', len(labeled_set))\n",
    "\n",
    "test_data_path = os.path.join(path, parent, 'Data/test/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoaders\n",
    "\n",
    "`torch.utils.data.DataLoader` takes care of splitting the data into minibatches of a given size, shuffling it if needed, and loading it in parallel by using multiprocessing workers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_loader = DataLoader(unlabeled_set, \n",
    "                              batch_size=16, \n",
    "                              shuffle=True, \n",
    "                              num_workers=0)\n",
    "\n",
    "labeled_loader = DataLoader(labeled_set, \n",
    "                            batch_size=16, \n",
    "                            shuffle=True, \n",
    "                            num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Augmentation\n",
    "\n",
    "With the `torchvision.transforms` package transformations applied at random with a chosen probability, augment the unlabeled and labeled data sets making sure that the augmented dataset follows the same distribution as the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_norm_parameters(dataset): \n",
    "    \"\"\" Function computing the mean and std for each chanel across the dataset \n",
    "        after stacking the tensors of the dataset \"\"\"\n",
    "\n",
    "    if len(dataset[0]) > 2:    \n",
    "        imgs = torch.stack([img_tensor for img_tensor in dataset], dim=3)\n",
    "        \n",
    "    else: \n",
    "        imgs = torch.stack([img_tensor for img_tensor, _ in dataset], dim=3)\n",
    "\n",
    "    #keep the RBG channels and merge the rest of the dims, 3x32x32 is transformed into 3x1024 \n",
    "    mean = imgs.view(3, -1).mean(dim=1)\n",
    "    std  = imgs.view(3, -1).std(dim=1)\n",
    "    \n",
    "    return mean, std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "unlabeled data set mean tensor([0.7722, 0.5286, 0.5501]) and std tensor([0.1399, 0.1602, 0.1772])\n",
      "labeled data set mean tensor([0.7441, 0.5331, 0.5548]) and std tensor([0.1464, 0.1557, 0.1735])\n"
     ]
    }
   ],
   "source": [
    "unlabeled_mean, unlabeled_std = get_norm_parameters(unlabeled_set)\n",
    "labeled_mean, labeled_std     = get_norm_parameters(labeled_set)\n",
    "\n",
    "print(\"unlabeled data set mean {} and std {}\".format(unlabeled_mean, unlabeled_std))\n",
    "print(\"labeled data set mean {} and std {}\".format(labeled_mean, labeled_std))\n",
    "\n",
    "#NB: The mean and std of the augmented data should be the same as the labeled!?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#augment the labeled data by applying randomly a list of transformations with given probability\n",
    "augmentation = transforms.Compose([transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                     transforms.RandomVerticalFlip(p=0.5),\n",
    "                                     transforms.RandomRotation(0,360)])\n",
    "                                    \n",
    "unlabeled_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                          transforms.Normalize(mean = unlabeled_mean,\n",
    "                                                               std = unlabeled_std)])                                   \n",
    "labeled_transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                          transforms.Normalize(mean = labeled_mean,\n",
    "                                                               std = labeled_std)])                                      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "unlabeled_set = UnlabeledDataset(unlabeled_data_path, \n",
    "                                 transform=transforms.Compose([augmentation, unlabeled_transform]))\n",
    "labeled_set = LabeledDataset(labeled_data_path,     \n",
    "                             transform=transforms.Compose([augmentation, labeled_transform]))\n",
    "                             \n",
    "test_set = LabeledDataset(test_data_path,\n",
    "                          transform=transforms.Compose([transforms.ToTensor()]))                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "num_workers = 0\n",
    "\n",
    "unlabeled_loader = DataLoader(unlabeled_set, \n",
    "                              batch_size=batch_size, \n",
    "                              num_workers=num_workers,\n",
    "                              shuffle=True)\n",
    "\n",
    "labeled_loader = DataLoader(labeled_set, \n",
    "                            batch_size=batch_size, \n",
    "                            num_workers=num_workers,\n",
    "                            shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(dataset = test_set,\n",
    "                         batch_size = batch_size,\n",
    "                         num_workers = num_workers,\n",
    "                         shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis of semi-supervised learning strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_0_1(imgs, nc=3):\n",
    "    \n",
    "    imgs = imgs.view(imgs.size(0), -1)\n",
    "    imgs -= imgs.min(1, keepdim=True)[0]\n",
    "    imgs /= imgs.max(1, keepdim=True)[0]\n",
    "    imgs = imgs.view(batch_size, nc , 32, 32)\n",
    "    \n",
    "    return imgs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of channels of training images\n",
    "nc = 3\n",
    "\n",
    "# Latent vector dimensions  latent vector, i.e. size of generator input\n",
    "z_dim = 20\n",
    "\n",
    "# Size of feature maps in generator\n",
    "ngf = 64\n",
    "\n",
    "# Size of feature maps in discriminator\n",
    "ndf = 64\n",
    "\n",
    "# Learning rate for optimizers\n",
    "lr = 0.001\n",
    "\n",
    "# Beta1 hyperparam for Adam optimizers\n",
    "beta1 = 0.5\n",
    "\n",
    "\n",
    "# Establish convention for real and fake labels during training\n",
    "real_label = 1\n",
    "fake_label = 0\n",
    "\n",
    "# batch of latent vectors used to visualize the progression of the generator\n",
    "fixed_noise = torch.randn(batch_size, z_dim, 1, 1, device=device)\n",
    "\n",
    "num_epochs = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weights_init(m, mean=0.0, std=0.2):\n",
    "    \n",
    "    \"\"\" Randomly initialize the weights of the Generator and Discriminator\n",
    "        to normaly distributed with mean of 0 and std of 0.2 \"\"\"\n",
    "    \n",
    "    classname = m.__class__.__name__\n",
    "    \n",
    "    if classname.find('Conv') != -1:\n",
    "        nn.init.normal_(m.weight.data, mean, std)\n",
    "        \n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        nn.init.normal_(m.weight.data, 1.0, std)\n",
    "        nn.init.constant_(m.bias.data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/elliemcintosh/Documents/Python/ManningBooks_LiveProjects/MelanomaDetection/Code'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(directory):\n",
    "    \n",
    "    \"\"\"Creates a directory if it does not already exist.\"\"\"\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_root = os.path.join(path, parent, 'Data/')\n",
    "gener_img_dir = \"generated_Melanoma\"\n",
    "check_pt_dir = \"/models_wts/Melanoma_SGAN/\"\n",
    "\n",
    "create_dir(data_root + gener_img_dir)\n",
    "create_dir(path + check_pt_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def checkpoint(epoch, G, D, optimizerG, optimizerD, lossG, lossD):\n",
    "    \"\"\"\n",
    "    Saves the parameters of the generator G and discriminator D\n",
    "    and other checkpoints for finetuning, Adam optimizer has internal\n",
    "    parameters which would be reset to default values\n",
    "    \"\"\"\n",
    "    Melanoma_SGAN_path = os.getcwd() + check_pt_dir + 'SGAN.pkl'  \n",
    "    # serializes the object with python pickle\n",
    "    torch.save({\n",
    "            'epoch': epoch,\n",
    "            'G_state_dict': G.state_dict(),\n",
    "            'D_state_dict': D.state_dict(),\n",
    "            'optimizerG_state_dict': optimizerG.state_dict(), #\n",
    "            'optimizerD_state_dict': optimizerD.state_dict(),\n",
    "            'lossG': lossG,\n",
    "            'lossD': lossD\n",
    "            }, Melanoma_SGAN_path)\n",
    "    \n",
    "def load_checkpoint(model, check_pt_name):\n",
    "    # unpickling to deserialize the saved object with torch.load; \n",
    "    model.load_state_dict(torch.load(os.getcwd() + check_pt_dir + check_pt_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = os.path.join(os.getcwd() + check_pt_dir + \"best_discriminator_wts.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "def binary_predict(x):\n",
    "    \n",
    "    \"\"\" Transoforms the 2 neurons output of the last fully\n",
    "        connected layer into a binary, real vs fake output\n",
    "        \n",
    "           D(x)= Z(x)/Z(x)+1 where Z(x)=Sum(exp[l(x)])\n",
    "           \n",
    "        where x is an input image, and l are the logits from \n",
    "        the final layer of the classifier. The sum over logits \n",
    "        goes from 1 to 2 (number of classes). The value of D(x)\n",
    "        is close to 1 when a strong prediction has been made for \n",
    "        one of the 2 (real) classes, and close to zero when the \n",
    "        2 logits' values are small.\"\"\"   \n",
    "    \n",
    "    # .sum(dim) Returns the sum of each row of the input tensor in the given dimension dim\n",
    "    # dim = -1 means the last dimension\n",
    "    Z = torch.exp(x).sum(dim = -1)\n",
    "    D = Z/(Z + 1.0)\n",
    "    \n",
    "    if torch.isnan(D).sum() > 0:\n",
    "        print(\"binary real vs fake prediction contains na: \", D)\n",
    "    \n",
    "    return D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "    \n",
    "    \n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(nc, ndf, kernel_size=2, stride=2, padding=2, bias=False)\n",
    "        self.conv2 = nn.Conv2d(ndf, ndf*2, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.batchn1 = nn.BatchNorm2d(ndf*2)\n",
    "        self.conv3 = nn.Conv2d(ndf*2, ndf*4, kernel_size=4, stride=2, padding=1, bias=False)\n",
    "        self.batchn2 = nn.BatchNorm2d(ndf*4)\n",
    "        self.conv4 = nn.Conv2d(ndf*4, ndf*2, kernel_size=4, stride=1, padding=0, bias=False)\n",
    "        \n",
    "        # fully connected layer with 2 output classes\n",
    "        self.fc1     = nn.Linear(ndf*2, 2, bias=True) \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, input):\n",
    "        x = self.conv1(input)\n",
    "        x = F.leaky_relu(x, 0.2, inplace=True)\n",
    "        x = self.conv2(x)\n",
    "        x = self.batchn1(x)\n",
    "        x = F.leaky_relu(x, 0.2, inplace=True)\n",
    "        x = self.conv3(x)\n",
    "        x = self.batchn2(x)\n",
    "        x = F.leaky_relu(x, 0.2, inplace=True)\n",
    "        x = self.conv4(x)\n",
    "        x = x.view(-1, ndf*2)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        \n",
    "        binary_out = binary_predict(x)\n",
    "        class_out  = self.softmax(x.squeeze())   \n",
    "        \n",
    "        return binary_out, class_out\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Discriminator(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(2, 2), stride=(2, 2), padding=(2, 2), bias=False)\n",
       "  (conv2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (batchn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv3): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "  (batchn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (conv4): Conv2d(256, 128, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
       "  (fc1): Linear(in_features=128, out_features=2, bias=True)\n",
       "  (softmax): Softmax(dim=1)\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create and initialize the generator and discriminator\n",
    "netG = Generator().to(device)\n",
    "netD = Discriminator().to(device)\n",
    "\n",
    "netG.apply(weights_init)\n",
    "netD.apply(weights_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the loss functions\n",
    "bce_loss = nn.BCELoss()\n",
    "cce_loss = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.8397, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "loss_supervised = cce_loss(labeled_class, labels)\n",
    "print(loss_supervised)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set up Adam optimizers for the Generator and Discriminator\n",
    "optimizerG = optim.Adam(netG.parameters(), lr = lr, betas=(beta1, 0.999))\n",
    "optimizerD = optim.Adam(netD.parameters(), lr = lr, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, device, data_loader):\n",
    "    \n",
    "    \"\"\"Evaluates the model's performance on test dataset\"\"\"\n",
    "\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "\n",
    "    test_acc = []\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        for imgs, labels in data_loader:\n",
    "            \n",
    "            \n",
    "            binary_output, class_output = netD(imgs.to(device))\n",
    "        \n",
    "            test_loss += cce_loss(class_output, labels.to(device)).item()\n",
    "            \n",
    "            pred = class_output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(labels.to(device).data.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    #print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(test_loss, correct, len(data_loader.dataset), 100. * correct/len(data_loader.dataset)))\n",
    "           \n",
    "    #return test_loss, 100.*correct/len(data_loader.dataset)\n",
    "            \n",
    "    avg_loss = np.round(test_loss/len(data_loader.dataset), 4)\n",
    "    accuracy = 100. * correct/len(data_loader.dataset)  \n",
    "    \n",
    "    print(\"Test Avg Loss: {:.4f}, Discriminator Test Accuracy: {:.2f} %\".format(avg_loss, accuracy))\n",
    "        \n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 37\n",
    "num_iter = np.ceil(len(labeled_loader.dataset)/batch_size).astype(int)\n",
    "print_every = 3\n",
    "\n",
    "fixed_fake_imgs =[]\n",
    "\n",
    "# Lists to keep track of progress\n",
    "G_losses = []\n",
    "D_losses = []\n",
    "\n",
    "epoch_losses = []\n",
    "train_loss = []\n",
    "train_cntr = []\n",
    "\n",
    "test_losses =[]\n",
    "test_accuracies =[]\n",
    "\n",
    "fixed_fake_list = []\n",
    "\n",
    "best_model_wts = torch.save(netD.state_dict(), \"best_discriminator_wts\")\n",
    "best_acc = 0.0\n",
    "\n",
    "displ_imgs = batch_size\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "            \n",
    "    #for each iteration\n",
    "    for i in range(num_iter):\n",
    "        \n",
    "        real_labeled_imgs, real_class_labels = next(iter(labeled_loader))\n",
    "        \n",
    "        real_unlabeled_imgs = next(iter(unlabeled_loader))\n",
    "        num_imgs = real_labeled_imgs.size()[0]\n",
    "        \n",
    "        real_labeled_imgs.to(device)\n",
    "        real_class_labels.to(device)\n",
    "        \n",
    "        real_unlabeled_imgs.to(device)  \n",
    "        \n",
    "        # make binary labeles for the real unlabeled images and for the fake generated images               \n",
    "        real_binary_labels = torch.full((num_imgs,), real_label, dtype=torch.float, device = device)\n",
    "        fake_binary_labels = torch.full((num_imgs,), fake_label, dtype=torch.float, device = device)\n",
    "               \n",
    "        ###############################################################\n",
    "        #                                                             #\n",
    "        #                      Train the Discriminator                #\n",
    "        #                                                             #\n",
    "        ###############################################################\n",
    "        \n",
    "        netD.train()\n",
    "        optimizerD.zero_grad()\n",
    "            \n",
    "        ################### On Unlabeled Images #######################\n",
    "        # rescale input images from [0, 1] to [-1, 1]\n",
    "        real_unlabeled_imgs = real_unlabeled_imgs * 2 - 1\n",
    "        \n",
    "        binary_real, _ = netD(real_unlabeled_imgs)\n",
    "        \n",
    "        binary_real_loss = 0.5 * bce_loss(binary_real, real_binary_labels)\n",
    "        \n",
    "        binary_real_loss.backward()      \n",
    "        \n",
    "        ################### On Generated Images ####################### \n",
    "        \n",
    "        noise = torch.randn(num_imgs, z_dim, 1, 1, device=device)\n",
    "        \n",
    "        fake_imgs = netG(noise)\n",
    "        \n",
    "        binary_fake, _ = netD(fake_imgs.detach()) \n",
    "        \n",
    "        binary_fake_loss = 0.5 * bce_loss(binary_fake, fake_binary_labels)\n",
    "        \n",
    "        binary_fake_loss.backward()\n",
    "        \n",
    "        loss_unsupervised = binary_real_loss + binary_fake_loss\n",
    "                \n",
    "        ####################### On Labeled Images ######################\n",
    "        # rescale input images from [0, 1] to [-1, 1]\n",
    "        real_labeled_imgs = real_labeled_imgs * 2 - 1\n",
    "        \n",
    "        _, class_real  = netD(real_labeled_imgs)\n",
    "        \n",
    "        # supervised loss\n",
    "        loss_supervised = cce_loss(class_real, real_class_labels)\n",
    "        \n",
    "        train_loss.append(loss_supervised.item())\n",
    "        train_cntr.append((i+1)*num_imgs + epoch * len(labeled_loader.dataset))\n",
    "  \n",
    "        loss_supervised.backward() #retain_graph=True\n",
    "        \n",
    "        ###################### update Discriminator ####################\n",
    "\n",
    "        lossD = loss_supervised + loss_unsupervised\n",
    "        \n",
    "        train_loss.append(lossD.item())\n",
    "        train_cntr.append((i+1)*num_imgs + epoch * len(labeled_loader.dataset))\n",
    "  \n",
    "        #lossD.backward(retain_graph=True)\n",
    "        \n",
    "        optimizerD.step()\n",
    "        \n",
    "        ###############################################################\n",
    "        #                                                             #\n",
    "        #                      Train the Generator                    #\n",
    "        #                                                             #\n",
    "        ###############################################################\n",
    "        \n",
    "        netG.train()\n",
    "        optimizerG.zero_grad()\n",
    "        \n",
    "        binary_fake, _ = netD(fake_imgs)\n",
    "        \n",
    "        lossG = bce_loss(binary_fake, real_binary_labels)\n",
    "        \n",
    "        lossG.backward()\n",
    "        \n",
    "        optimizerG.step()      \n",
    "                \n",
    "        ###############################################################\n",
    "        #                                                             #\n",
    "        #                    Output training stats                    #\n",
    "        #                                                             #\n",
    "        ###############################################################\n",
    "        \n",
    "        # Calculate discriminator accuracy\n",
    "        pred_labels = np.concatenate([class_real.data.cpu().numpy(), ], axis=0)\n",
    "        ground_truth = np.concatenate([real_class_labels.data.cpu().numpy(), ], axis=0)\n",
    "        \n",
    "        # get the indices of the maximum values along the 2 class dim 1\n",
    "        D_acc = np.mean(np.argmax(pred_labels, axis=1) == ground_truth)\n",
    "        # print out training stats every chosen number of batches\n",
    "        if i % print_every == 0:\n",
    "            \n",
    "            print('\\nEpoch {}/{}  Batch {}/{}'.format(epoch +1, num_epochs, i, len(labeled_loader)))\n",
    "            print('-'*30)\n",
    "            print('Classifier Loss: {:.4f}, Generator Loss: {:.4f}, Training Accuracy: {:.1f} %'.format(loss_supervised.item(), lossG.item(), 100*D_acc))\n",
    "            print()\n",
    "            \n",
    "        # save losses for plotting\n",
    "        G_losses.append(lossG.item())\n",
    "        D_losses.append(lossD.item())   \n",
    "    \n",
    "    ############################ after each epoch  #########################\n",
    "    \n",
    "    #evaluate how the generator is doing\n",
    "    #netG.eval()\n",
    "    #with torch.no_grad():\n",
    "        #rescale from [-1, 1] back to [0, 1]\n",
    "        #fixed_fake = (netG(fixed_noise).detach().cpu()+1.0)/2.0\n",
    "        #image_grid = torchvision.utils.make_grid(convert_0_1(fixed_fake), nrow=16)\n",
    "        \n",
    "        #print(\"Epoch \", epoch +1)\n",
    "        #plt.figure(figsize=(20,10))\n",
    "        #plt.imshow(image_grid.permute(1, 2, 0))\n",
    "        #plt.axis(\"off\")\n",
    "        #plt.pause(0.05)\n",
    "        \n",
    "        #fixed_fake_list.append(torchvision.utils.make_grid(convert_0_1(fixed_fake.detach().cpu()), padding=2, normalize=True, nrow=16))\n",
    "        \n",
    "        #torchvision.utils.save_image(fixed_fake.data[:], \n",
    "                                     #data_root + gener_img_dir + \"/fixed_fake_%d.jpg\" % (epoch+1), \n",
    "                                     #nrow=16, normalize=True)\n",
    "    \n",
    "    #netG.train()\n",
    "    \n",
    "    # evaluate how the Discriminator is doing on training and testing datasets\n",
    "    epoch_losses.append(train_loss[-1])\n",
    "    \n",
    "    print(\"\\nEpoch: {}, Loss {:.4f}, Discriminator Avg Training Loss {:.4f}\".format(epoch+1, epoch_losses[-1], sum(epoch_losses)/len(epoch_losses)))\n",
    "    \n",
    "    test_loss, test_accuracy = test(netD, device, test_loader)\n",
    "    test_losses.append(test_loss)\n",
    "    test_accuracies.append(test_accuracy)\n",
    "    \n",
    "    if test_accuracy > best_acc:\n",
    "        best_acc = test_accuracy\n",
    "        best_discriminator_wts = torch.save(netD.state_dict(), os.getcwd() + check_pt_dir + \"best_discriminator_wts.pkl\")\n",
    "    \n",
    "    print(\"Discriminator best test accuracy {:.2f} %\".format(best_acc))\n",
    "        \n",
    "    checkpoint(epoch, netG, netD, optimizerG, optimizerD, G_losses, D_losses)\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Iterations')"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot Discriminator as a supervised classifier training performance\n",
    "plt.plot(train_cntr, train_loss, color='blue')\n",
    "plt.title(\"Supervised Classifier on 200 trainig samples\")\n",
    "plt.ylabel('Train loss')\n",
    "plt.xlabel('Iterations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(num_epochs), np.array(epoch_losses))\n",
    "plt.ylabel('Train loss')\n",
    "plt.xlabel('epochs')\n",
    "plt.title(\"Discriminator train and test performance\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax1 = plt.subplots()\n",
    "\n",
    "color = 'tab:red'\n",
    "ax1.set_xlabel('epochs')\n",
    "ax1.set_ylabel('Test loss', color=color)\n",
    "ax1.plot(range(num_epochs), np.array(test_losses), color=color)\n",
    "ax1.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "ax2 = ax1.twinx() \n",
    "\n",
    "color = 'tab:blue'\n",
    "ax2.set_ylabel('Test accuracy', color=color)  \n",
    "ax2.plot(range(num_epochs), np.array(test_accuracies), color=color)\n",
    "ax2.tick_params(axis='y', labelcolor=color)\n",
    "\n",
    "fig.tight_layout()  \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load best model weights\n",
    "netD.load_state_dict(torch.load(best_model_path))\n",
    "\n",
    "print(\"\\nBest model on the test set: \")\n",
    "test(netD, device, test_loader) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
